{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron (MLP)\n",
    "In this homework you are required to implement and train a 3-layer neural network to classify images of hand-written digits from the MNIST dataset. The input to the network will be a 28 Ã— 28-pixel image, which is converted into a 784-dimensional vector. The output will be a vector of 10 probabilities (one for each digit).\n",
    "![jupyter](./mlp.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Forward Propagation**: Compute the intermediate outputs $\\mathbf{z}_{1}$, $\\mathbf{h}_{1}$, $\\mathbf{z}_{2}$, and $\\hat{\\mathbf{y}}$ as the directed graph shown below:\n",
    "Specifically, the network you create should implement a function $g: \\mathbb{R}^{784} \\rightarrow \\mathbb{R}^{10}$, where:\n",
    "\n",
    "$$\\mathbf{z}_{1} = \\mathbf{W}^{(1)}\\mathbf{x} + \\mathbf{b}^{(1)}$$\n",
    "$$\\mathbf{h}_1 = ReLU(\\mathbf{z}_1)$$\n",
    "$$\\mathbf{z}_2 = \\mathbf{W}^{(2)}\\mathbf{h}_1 + \\mathbf{b}^{(2)}$$\n",
    "$$\\hat{\\mathbf{y}} = g(\\mathbf{x}) = Softmax(\\mathbf{z}_2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Loss function**: After forward propagation, you should use the cross-entropy loss function: \n",
    "$$ f_{CE}(\\mathbf{W}^{(1)},\\mathbf{b}^{(1)}, \\mathbf{W}^{(2)}, \\mathbf{b}^{(2)}) =  - \\frac{1}{n}\\sum_{i=1}^{n} \\sum_{k=1}^{10} \\mathbf{y}_k^{(i)} \\log \\hat{\\mathbf{y}}_k^{(i)} $$\n",
    "where $n$ is the number of examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Backwards Propagation**: The individual gradient for each parameter term can be shown as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial f_{CE}}{\\partial \\mathbf{W}^{(2)}}  =  \\frac{1}{n}\\sum_{i=1}^{n}  (\\hat{\\mathbf{y}}^{(i)} - \\mathbf{y}^{(i)})  (\\mathbf{h_1}^{(i)})^{T}  $$\n",
    "\n",
    "\n",
    "$$ \\frac{\\partial f_{CE}}{\\partial \\mathbf{b}^{(2)}} =  \\frac{1}{n}\\sum_{i=1}^{n}  (\\hat{\\mathbf{y}}^{(i)} - \\mathbf{y}^{(i)})   $$\n",
    "\n",
    "$$ \\frac{\\partial f_{CE}}{\\partial \\mathbf{W}^{(1)}} = \\frac{1}{n}\\sum_{i=1}^{n} {\\mathbf{W}^{(2)}}^{T}(\\hat{\\mathbf{y}}^{(i)} - \\mathbf{y}^{(i)}) \\circ sgn(\\mathbf{z_1}^{(i)}) (\\mathbf{x}^{(i)})^{T}$$ \n",
    "    \n",
    "    \n",
    "$$ \\frac{\\partial f_{CE}}{\\partial \\mathbf{b}^{(1)}} =  \\frac{1}{n}\\sum_{i=1}^{n} {\\mathbf{W}^{(2)}}^{T}(\\hat{\\mathbf{y}}^{(i)} - \\mathbf{y}^{(i)}) \\circ sgn(\\mathbf{z_1}^{(i)})   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your tasks: \n",
    "1. Implement stochastic gradient descent for the network shown above with the help of the starter code. Specially, you need to finish the code of three functions: fCE, gradCE, train.\n",
    "\n",
    "2. Train the network using proper hyper-parameters (batch size, learning rate etc), and report the train accuracy and test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_HIDDEN:  50\n",
      "LEARNING_RATE:  0.05\n",
      "BATCH_SIZE:  64\n",
      "NUM_EPOCH:  40\n",
      "len(trainX):  10000\n",
      "len(testX):  5000\n",
      "(39760,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits.mplot3d import axes3d, Axes3D\n",
    "\n",
    "\n",
    "## Network architecture\n",
    "NUM_INPUT = 784  # Number of input neurons\n",
    "NUM_OUTPUT = 10  # Number of output neurons\n",
    "NUM_CHECK = 5  # Number of examples on which to check the gradient\n",
    "\n",
    "## Hyperparameters\n",
    "NUM_HIDDEN = 50\n",
    "LEARNING_RATE = 0.05\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCH = 40\n",
    "\n",
    "print(\"NUM_HIDDEN: \", NUM_HIDDEN)\n",
    "print(\"LEARNING_RATE: \", LEARNING_RATE)\n",
    "print(\"BATCH_SIZE: \", BATCH_SIZE)\n",
    "print(\"NUM_EPOCH: \", NUM_EPOCH)\n",
    "\n",
    "# Given a vector w containing all the weights and biased vectors, extract\n",
    "# and return the individual weights and biases W1, b1, W2, b2.\n",
    "def unpack (w):\n",
    "    W1 = np.reshape(w[:NUM_INPUT * NUM_HIDDEN],(NUM_INPUT,NUM_HIDDEN))\n",
    "    w = w[NUM_INPUT * NUM_HIDDEN:]\n",
    "    b1 = np.reshape(w[:NUM_HIDDEN], NUM_HIDDEN)\n",
    "    w = w[NUM_HIDDEN:]\n",
    "    W2 = np.reshape(w[:NUM_HIDDEN*NUM_OUTPUT], (NUM_HIDDEN,NUM_OUTPUT))\n",
    "    w = w[NUM_HIDDEN*NUM_OUTPUT:]\n",
    "    b2 = np.reshape(w,NUM_OUTPUT)\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "# Given individual weights and biases W1, b1, W2, b2, concatenate them and\n",
    "# return a vector w containing all of them.\n",
    "def pack (W1, b1, W2, b2):\n",
    "    W1_ = np.reshape(W1,NUM_INPUT*NUM_HIDDEN)\n",
    "    # print(W1_.shape)\n",
    "    W2_ = np.reshape(W2,NUM_HIDDEN*NUM_OUTPUT)\n",
    "    # print(W2_.shape)\n",
    "    w = np.concatenate((W1_,b1, W2_, b2))\n",
    "    # print(w.shape)\n",
    "    return w\n",
    "\n",
    "# Load the images and labels from a specified dataset (train or test).\n",
    "def loadData (which):\n",
    "    images = np.load(\"./data/mnist_{}_images.npy\".format(which))\n",
    "    labels = np.load(\"./data/mnist_{}_labels.npy\".format(which))\n",
    "    return images, labels\n",
    "\n",
    "## 1. Forward Propagation\n",
    "# Given training images X, associated labels Y, and a vector of combined weights\n",
    "# and bias terms w, compute and return the cross-entropy (CE) loss.\n",
    "\n",
    "def fCE (X, Y, w):\n",
    "    # print(X.shape)\n",
    "    W1, b1, W2, b2 = unpack(w)\n",
    "    loss = 0.0\n",
    "    ## your code here\n",
    "\n",
    "    return loss\n",
    "\n",
    "## 2. Backward Propagation\n",
    "# Given training images X, associated labels Y, and a vector of combined weights\n",
    "# and bias terms w, compute and return the gradient of fCE. \n",
    "def gradCE (X, Y, w):\n",
    "    W1, b1, W2, b2 = unpack(w)\n",
    "    ## your code here\n",
    "    \n",
    "    delta = pack(delta_W_1, delta_b_1, delta_W_2, delta_b_2)\n",
    "    return delta\n",
    "\n",
    "## 3. Parameter Update\n",
    "# Given training and testing datasets and an initial set of weights/biases b,\n",
    "# train the NN.\n",
    "def train(trainX, trainY, testX, testY, w):\n",
    "    ## your code here\n",
    "    pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    start_time = time.time()\n",
    "    trainX, trainY = loadData(\"train\")\n",
    "    testX, testY = loadData(\"test\")\n",
    "\n",
    "    print(\"len(trainX): \", len(trainX))\n",
    "    print(\"len(testX): \", len(testX))\n",
    "\n",
    "    # Initialize weights randomly\n",
    "    W1 = 2*(np.random.random(size=(NUM_INPUT, NUM_HIDDEN))/NUM_INPUT**0.5) - 1./NUM_INPUT**0.5\n",
    "    b1 = 0.01 * np.ones(NUM_HIDDEN)\n",
    "    W2 = 2*(np.random.random(size=(NUM_HIDDEN, NUM_OUTPUT))/NUM_HIDDEN**0.5) - 1./NUM_HIDDEN**0.5\n",
    "    b2 = 0.01 * np.ones(NUM_OUTPUT)\n",
    "\n",
    "    w = pack(W1, b1, W2, b2)\n",
    "    print(w.shape)\n",
    "\n",
    "    W1_, b1_, W2_, b2_ = unpack(w)\n",
    "\n",
    "    # # Train the network and report the accuracy on the training and test set.\n",
    "    ws = train(trainX, trainY, testX, testY, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
