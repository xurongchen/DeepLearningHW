{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "NUM_HIDDEN:  50\nLEARNING_RATE:  0.05\nBATCH_SIZE:  64\nNUM_EPOCH:  400\nlen(trainX):  10000\nlen(testX):  5000\nShape of w: (39760,)\nTrain Acc:0.9999; Test Acc:0.9998\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "\n",
    "## Network architecture\n",
    "NUM_INPUT = 784  # Number of input neurons\n",
    "NUM_OUTPUT = 10  # Number of output neurons\n",
    "NUM_CHECK = 5  # Number of examples on which to check the gradient\n",
    "\n",
    "## Hyperparameters\n",
    "NUM_HIDDEN = 50\n",
    "LEARNING_RATE = 0.05\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCH = 400\n",
    "\n",
    "print(\"NUM_HIDDEN: \", NUM_HIDDEN)\n",
    "print(\"LEARNING_RATE: \", LEARNING_RATE)\n",
    "print(\"BATCH_SIZE: \", BATCH_SIZE)\n",
    "print(\"NUM_EPOCH: \", NUM_EPOCH)\n",
    "\n",
    "# Given a vector w containing all the weights and biased vectors, extract\n",
    "# and return the individual weights and biases W1, b1, W2, b2.\n",
    "def unpack (w):\n",
    "    W1 = np.reshape(w[:NUM_INPUT * NUM_HIDDEN],(NUM_INPUT,NUM_HIDDEN))\n",
    "    w = w[NUM_INPUT * NUM_HIDDEN:]\n",
    "    b1 = np.reshape(w[:NUM_HIDDEN], NUM_HIDDEN)\n",
    "    w = w[NUM_HIDDEN:]\n",
    "    W2 = np.reshape(w[:NUM_HIDDEN*NUM_OUTPUT], (NUM_HIDDEN,NUM_OUTPUT))\n",
    "    w = w[NUM_HIDDEN*NUM_OUTPUT:]\n",
    "    b2 = np.reshape(w,NUM_OUTPUT)\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "# Given individual weights and biases W1, b1, W2, b2, concatenate them and\n",
    "# return a vector w containing all of them.\n",
    "def pack (W1, b1, W2, b2):\n",
    "    W1_ = np.reshape(W1,NUM_INPUT*NUM_HIDDEN)\n",
    "    # print(W1_.shape)\n",
    "    W2_ = np.reshape(W2,NUM_HIDDEN*NUM_OUTPUT)\n",
    "    # print(W2_.shape)\n",
    "    w = np.concatenate((W1_,b1, W2_, b2))\n",
    "    # print(w.shape)\n",
    "    return w\n",
    "\n",
    "# Load the images and labels from a specified dataset (train or test).\n",
    "def loadData (which):\n",
    "    images = np.load(\"./data/mnist_{}_images.npy\".format(which))\n",
    "    labels = np.load(\"./data/mnist_{}_labels.npy\".format(which))\n",
    "    return images, labels\n",
    "\n",
    "## 1. Forward Propagation\n",
    "# Given training images X, associated labels Y, and a vector of combined weights\n",
    "# and bias terms w, compute and return the cross-entropy (CE) loss.\n",
    "def ReLU(X):\n",
    "   return np.maximum(0,X)\n",
    "\n",
    "def Softmax(X):\n",
    "    # expo = np.exp(X)\n",
    "    # expo_sum = np.sum(np.exp(X))\n",
    "    # return expo/expo_sum\n",
    "    ### Compute the softmax in a numerically stable way.\n",
    "    X = X - np.max(X)\n",
    "    exp_x = np.exp(X)\n",
    "    softmax_x = exp_x / np.sum(exp_x)\n",
    "    return softmax_x\n",
    "    \n",
    "def Sgn(X):\n",
    "    return np.signbit(X).astype(int)\n",
    "\n",
    "def fCE (X, Y, w):\n",
    "    # print(X.shape)\n",
    "    W1, b1, W2, b2 = unpack(w)\n",
    "    loss = 0.0\n",
    "    ## your code here\n",
    "    z1 = X.dot(W1) + b1\n",
    "    h1 = ReLU(z1)\n",
    "    z2 = h1.dot(W2) + b2\n",
    "    Yhat = Softmax(z2)\n",
    "    loss = -np.sum(Y * np.log(Yhat+1e-7))/Y.shape[0]\n",
    "    return loss,Yhat,h1,z1\n",
    "\n",
    "## 2. Backward Propagation\n",
    "# Given training images X, associated labels Y, and a vector of combined weights\n",
    "# and bias terms w, compute and return the gradient of fCE. \n",
    "def gradCE (X, Y, w, Yhat, h1, z1):\n",
    "    W1, b1, W2, b2 = unpack(w)\n",
    "    ## your code here\n",
    "    delta_W_2 = np.dot(h1.T, Yhat - Y) / Y.shape[0]\n",
    "    delta_b_2 = np.sum(Yhat - Y, axis=0) / Y.shape[0]\n",
    "    delta_W_1 = np.dot(X.T, np.dot(Yhat - Y, W2.T) * Sgn(z1)) / Y.shape[0]\n",
    "    delta_b_1 = np.sum(np.dot(Yhat - Y, W2.T) * Sgn(z1), axis=0) / Y.shape[0]\n",
    "    delta = pack(delta_W_1, delta_b_1, delta_W_2, delta_b_2)\n",
    "    return delta\n",
    "\n",
    "## 3. Parameter Update\n",
    "# Given training and testing datasets and an initial set of weights/biases,\n",
    "# train the NN.\n",
    "def train(trainX, trainY, testX, testY, w):\n",
    "    ## your code here\n",
    "    learnRate = LEARNING_RATE\n",
    "    for epochRound in range(NUM_EPOCH):\n",
    "        batchIter = get_data_batch(trainX, trainY, BATCH_SIZE, True)\n",
    "        while True:\n",
    "            batch_X, batch_Y = next(batchIter)\n",
    "            if len(batch_X) == 0:\n",
    "                break\n",
    "            loss,Yhat,h1,z1 = fCE(batch_X,batch_Y,w)\n",
    "           \n",
    "            delta = gradCE(batch_X,batch_Y,w,Yhat,h1,z1)\n",
    "            W1, b1, W2, b2 = unpack(w)\n",
    "            delta_W_1, delta_b_1, delta_W_2, delta_b_2 = unpack(delta)\n",
    "            W1 = W1 - delta_W_1 * learnRate\n",
    "            b1 = b1 - delta_b_1 * learnRate\n",
    "            W2 = W2 - delta_W_2 * learnRate\n",
    "            b2 = b2 - delta_b_2 * learnRate\n",
    "            w = pack(W1, b1, W2, b2)\n",
    "            # print(loss)\n",
    "\n",
    "    lossTr,YhatTr,_,_ = fCE(trainX,trainY,w)\n",
    "    lossTe,YhatTe,_,_ = fCE(testX,testY,w)\n",
    "    PredictTr = list(map(lambda x: x == max(x), YhatTr)) * np.ones(shape=YhatTr.shape)\n",
    "    PredictTe = list(map(lambda x: x == max(x), YhatTe)) * np.ones(shape=YhatTe.shape)\n",
    "    accTr = sum(sum(PredictTr * trainY)) / len(trainY)\n",
    "    accTe = sum(sum(PredictTe * testY)) / len(testY)\n",
    "    print(\"Train Acc:{0}; Test Acc:{1}\".format(accTr,accTe))\n",
    "\n",
    "def get_data_batch(X, Y, batch_size=None, shuffle=False):\n",
    "    size = len(X[0])\n",
    "    indices = list(range(size))\n",
    "    if shuffle:\n",
    "        random.shuffle(indices)\n",
    "    while True:\n",
    "        batch_indices = np.asarray(indices[0:batch_size])\n",
    "        indices = indices[batch_size:] #+ indices[:batch_size]\n",
    "        if len(indices) == 0:\n",
    "            yield [],[]\n",
    "        batch_X = X[batch_indices]\n",
    "        batch_Y = Y[batch_indices]\n",
    "        yield batch_X,batch_Y\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    start_time = time.time()\n",
    "    trainX, trainY = loadData(\"train\")\n",
    "    testX, testY = loadData(\"test\")\n",
    "\n",
    "    print(\"len(trainX): \", len(trainX))\n",
    "    print(\"len(testX): \", len(testX))\n",
    "\n",
    "    # Initialize weights randomly\n",
    "    W1 = 2*(np.random.random(size=(NUM_INPUT, NUM_HIDDEN))/NUM_INPUT**0.5) - 1./NUM_INPUT**0.5\n",
    "    b1 = 0.01 * np.ones(NUM_HIDDEN)\n",
    "    W2 = 2*(np.random.random(size=(NUM_HIDDEN, NUM_OUTPUT))/NUM_HIDDEN**0.5) - 1./NUM_HIDDEN**0.5\n",
    "    b2 = 0.01 * np.ones(NUM_OUTPUT)\n",
    "\n",
    "    w = pack(W1, b1, W2, b2)\n",
    "    print(\"Shape of w:\",w.shape)\n",
    "\n",
    "    # # Train the network and report the accuracy on the training and test set.\n",
    "    train(trainX, trainY, testX, testY, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python37464bit01a808e8a176457a80e03a777f78859b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}